\documentclass{article}

\usepackage{fullpage}
\usepackage{color}
\usepackage{amsmath}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{listings} % For displaying code
\usepackage{jlcode} % for Julia code styling

\graphicspath{ {./images/} }

% Answers
\def\ans#1{\par\gre{Answer: #1}}

% Colors
\definecolor{blu}{rgb}{0,0,1}
\def\blu#1{{\color{blu}#1}}
\definecolor{gre}{rgb}{0,.5,0}
\def\gre#1{{\color{gre}#1}}
\definecolor{red}{rgb}{1,0,0}
\def\red#1{{\color{red}#1}}
\def\norm#1{\|#1\|}

% Math
\def\R{\mathbb{R}}
\def\argmax{\mathop{\rm arg\,max}}
\def\argmin{\mathop{\rm arg\,min}}
\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\alignStar}[1]{\begin{align*}#1\end{align*}}

% LaTeX
\newcommand{\fig}[2]{\includegraphics[width=#1\textwidth]{a1f/#2}}
\newcommand{\centerfig}[2]{\begin{center}\includegraphics[width=#1\textwidth]{a1f/#2}\end{center}}
\def\items#1{\begin{itemize}#1\end{itemize}}
\def\enum#1{\begin{enumerate}#1\end{enumerate}}



\begin{document}
\lstset{language=julia}

\title{CPSC 340 Assignment 1 (due Friday September 12 at 11:55pm)}
% \author{Pooya Daravi - S\#:34584145}
\date{}
\maketitle
\vspace{-4em}


\section*{Basic Information}


\blu{\enum{
\item Name: Pooya Daravi
\item Student ID: 34584145
}
}


\section{Linear Algebra Review}
\subsection{Basic Operations}

Use the definitions below,
\[
\alpha = 2,\quad
x = \left[\begin{array}{c}
0\\
1\\
2\\
\end{array}\right], \quad 
y = \left[\begin{array}{c}
3\\
4\\
5\\
\end{array}\right],\quad
z = \left[\begin{array}{c}
1\\
2\\
-1\\
\end{array}\right],\quad
A = \left[\begin{array}{ccc}
3 & 2 & 2\\
1 & 3 & 1\\
1 & 1 & 3
\end{array}\right],
\]
and use $x_i$ to denote element $i$ of vector $x$.
\blu{Evaluate the following expresions} (you do not need to show your work).

\ans{
    \enum{
    \item $\sum_{i=1}^n x_iy_i$ = 14
    \item $\sum_{i=1}^n x_iz_i$ = 0
    \item $\alpha(x+y)$ = \[ \begin{bmatrix} 6 \\ 10 \\ 15 \end{bmatrix} \]
    \item $\norm{x}$ = 5
    \item $x^T$ = \[ \begin{bmatrix} 0 & 1 & 2 \end{bmatrix} \]
    \item $Ax$ = \[ \begin{bmatrix} 6 \\ 5 \\ 7 \end{bmatrix} \]
    \item $x^TAx$ = 19
    }
}


\subsection{Matrix Algebra Rules}

Assume that $\{x,y,z\}$ are $n \times 1$ column vectors and $\{A,B,C\}$ are $n \times n$ real-valued matrices, $0$ is the zero matrix of appropriate size, and $I$ is the identity matrix of appropriate size. \blu{State whether each of the below is true in general} (you do not need to show your work).

\ans{
	\begin{enumerate}
		\item $x^Ty = \sum_{i=1}^n x_iy_i$. ----- True
		\item $x^Tx = \norm{x}^2$. ----- True
		\item $x^Tx = xx^T$. ----- False
		\item $(x-y)^T(y-x) = \norm{x}^2 - 2x^\top y + \norm{y}^2$. ----- False
		\item $AB=BA$. ----- False
		\item $A(B + C) = AB + AC$. ----- True
		\item $(AB)^T = A^TB^T$. ----- False
		\item $x^TAy = y^TA^Tx$. ----- False
		\item $A^\top A = I$ is the columns of $A$ are orthonormal. ----- True ???
	\end{enumerate}
}


\section{Probability Review}
\subsection{Rules of probability}

\blu{Answer the following questions.} You do not need to show your work.

\begin{enumerate}
\item You are offered the opportunity to play the following game: your opponent rolls 2 regular 6-sided dice. If the difference between the two rolls is at least 3, you win \$15. Otherwise, you get nothing. What is a fair price for a ticket to play this game once? In other words, what is the expected value of playing the game?
\ans{\$5}
\item Consider two events $A$ and $B$ such that $\Pr(A, B)=0$ (they are mutually exclusive). If $\Pr(A) = 0.4$ and $\Pr(A \cup B) = 0.95$, what is $\Pr(B)$? Note: $p(A, B)$ means
``probability of $A$ and $B$'' while $p(A \cup B)$ means ``probability of $A$ or $B$''. It may be helpful to draw a Venn diagram.
\ans{0.55}
\item Instead of assuming that $A$ and $B$ are mutually exclusive ($\Pr(A,B) = 0)$, what is the answer to the previous question if we assume that $A$ and $B$ are independent?
\ans{0.92}
\end{enumerate}

\subsection{Bayes Rule and Conditional Probability}

\blu{Answer the following questions.} You do not need to show your work.

Suppose a drug test produces a positive result with probability $0.95$ for drug users, $P(T=1|D=1)=0.95$. It also produces a negative result with probability $0.99$ for non-drug users, $P(T=0|D=0)=0.99$. The probability that a random person uses the drug is $0.0001$, so $P(D=1)=0.0001$. 

\begin{enumerate}
\item What is the probability that a random person would test positive, $P(T=1)$?
\item In the above, do most of these positive tests come from true positives or from false positives? 
\item What is the probability that a random person who tests positive is a user, $P(D=1|T=1)$?
\item Suppose you have given this test to a random person and it came back positive, are they likely to be a drug user?
\item What is one factor you could change to make this a more useful test?
\end{enumerate}



\section{Calculus Review}

For these questions you may find it helpful to review these notes on calculus:\\
\url{http://www.cs.ubc.ca/~schmidtm/Courses/Notes/calculus.pdf}\\


\subsection{One-variable derivatives}

\blu{Answer the following questions.} You do not need to show your work.

\begin{enumerate}
\item Find the derivative of the function $f(x) = 3x^2 -2x + 5$.
\item Find the derivative of the function $f(x) = x(1-x)$.
\item Let $p(x) = \frac{1}{1+\exp(-x)}$ for $x \in \R$. Compute the derivative of the function $f(x) = x-\log(p(x))$ and simplify it by using the function $p(x)$.
\end{enumerate}
Note that in this course we will use $\log(x)$ to mean the ``natural'' logarithm of $x$, so that $\log(\exp(1)) = 1$. Also, obseve that $p(x) = 1-p(-x)$ for the final part.

\subsection{Multi-variable derivatives}

\blu{Compute the gradient $\nabla f(x)$ of each of the following functions.} You do not need to show your work.
\begin{enumerate}
% \item $f_1(x) = \sin(x)$
\item $f(x) = x_1^2 + \exp(x_2)$ where $x \in \R^2$.
\item $f(x) = \exp(x_1 + x_2x_3)$ where $x \in \mathbb{R}^3$.
\item $f(x) = a^Tx$ where $x \in \R^2$ and $a \in \R^2$.
\item $f(x) = x^\top A x$ where $A=\left[ \begin{array}{cc}
2 & -1 \\
 -1 & 1 \end{array} \right]$ and $x \in \mathbb{R}^2$.
 \item $f(x) = \frac{1}{2}\norm{x}^2$ where $x \in \R^d$.
\end{enumerate}

Hint: it is helpful to write out the linear algebra expressions in terms of summations.



\subsection{Derivatives of code}

\begin{lstlisting}[language=julia, frame=single]
function grad1(x)
	n = length(x);
	g = zeros(n);
	for i in 1:n
		g[i] = 3 * (x[i]^2);
	end
	return g;
end

function grad2(x)
	n = length(x);
	g = zeros(n);
	for i in 1:n
		g[i] = prod(x) / x[i];
	end
	return g;
end

function grad3(x)
	n = length(x);
	g = zeros(n)
	for i in 1:n
		g[i] = exp(-x[i]) / (1 + exp(-x[i]));
	end
	return g;
end
\end{lstlisting}

\section{Algorithms and Data Structures Review}

For these questions you may find it helpful to review these notes on big-O notation:\\
\url{http://www.cs.ubc.ca/~schmidtm/Courses/Notes/bigO.pdf}

\subsection{Trees}

\blu{Answer the following questions} You do not need to show your work.
\begin{enumerate}
\item What is the maximum number of \emph{leaves} you could have in a binary tree of depth $l$?
\item What is the maximum number of \emph{internal nodes} (excluding leaves) you could have in a binary tree of depth $l$?
\end{enumerate}
Note: we'll use the standard convention that the leaves are not included in the depth, so a tree with depth $1$ has 3 nodes with 2 leaves.


\subsection{Common Runtimes}

\blu{Answer the following questions using big-$O$ notation} You do not need to show your work.
\begin{enumerate}
\item What is the cost of running the mergesort algorithm to sort  a list of $n$ numbers?
\item What is the cost of finding the third-largest element of an unsorted list of $n$ numbers?
\item What is the cost of finding the smallest element greater than 0 in a \emph{sorted} list with $n$ numbers?
\item What is the cost of finding the value associated with a key in a hash table with $n$ numbers? \\(Assume the values and keys are both scalars.)
\item What is the cost of computing the matrix-vector product $Ax$ when $A$ is $n \times d$ and $x$ is $d \times 1$?
\item What is the cost of computing the quadratic form $x^TAx$ when $A$ is $d \times d$ and $x$ is $d \times 1$?
\item How does the answer to the previous question change if $A$ has only $z$ non-zeroes? (You can assume $z \geq d$)
\end{enumerate}

\subsection{Running times of code}

Included in \texttt{a1.zip} is file named \texttt{bigO.jl}, which defines several functions
that take an integer argument $n$. For each function, \blu{state the running time as a function of $n$, using big-O notation}.

\ans{
	\begin{enumerate}
		\item T(func1) = $O(1)$
		\item T(func2) = $O(1)$
		\item T(func3) = $O(n)$
		\item T(func4) = $O(n^2)$
	\end{enumerate}
}

\section{Summary Statistics and Data Visualization}

\subsection{Summary Statistics}

\blu{Report the following statistics}: the minimum, maximum, mean, median, and mode of all values across the dataset.
\ans{
	\begin{enumerate}
		\item minimum: 0.352
		\item maximum: 4.862
		\item mean: 1.325
		\item median: 1.159
		\item mode: 0.77
	\end{enumerate}
}

In light of thea above, \blu{is the mode a reliable estimate of the most ``common" value? Describe another way we could give a meaningful ``mode" measurement for this (continuous) data.}

\ans{
	No. Since our data freqeuencies are very small compared to the different possible values, the mode does not meaningfully represent commonality. To have a more meaningful mode we could re-code our data into bins large enough that the result does have frequency values comparable to the number of bins. And therefore the mode would be representing a significant portion of the data and be an appropriate statistical measure of commonality.
}

\subsection{Data Visualization}

Consider the figure on the next page.
The figure contains the following plots, in a shuffled order:
\enum{
\item A histogram showing the distribution of all values in the matrix $X$.
\item A boxplot grouping data by weeks, showing the distribution across regions for each week.
\item A scatterplot between the two regions with highest correlation.
\item A single histogram showing the distribution of \emph{each} column in $X$.
\item A scatterplot between the two regions with lowest correlation.
\item A plot containing the weeks on the $x$-axis and the percentages for each region on the $y$-axis.
}
\blu{Match the plots (labeled A-F) with the descriptions above (labeled 1-6), with an extremely brief (a few words is fine) explanation for each decision.}

\ans{
	\begin{enumerate}
		\item A -> 6 --- Axes match the description and each coloured line corresponds to a region
		\item B -> 2 --- The only boxplot graph! (which also matches the description)
		\item C -> 1 --- Simplest histogram that matches the description
		\item D -> 4 --- Each colour is a histogram of the corresponding column
		\item E -> 5 --- Less correlation -> data spread more as opposed to being linear
		\item F -> 3 --- High correlation -> closer to a straight line
	\end{enumerate}
}

\subsection{Decision Surfaces}

\blu{How many training examples has the neural network mis-classified?}
\ans{
	17
}


\section{Decision Trees}

\subsection{Equality vs. Inequality Splitting Rules}

In class we discussed splitting rules based on inequalities rather than equalities. \blu{Is there a type of feature where it makes sense to use 
an equality-based splitting rule?}

\ans{
	Yes. For categorical or binary features it makes sense to use equality rather than inequlity. Or where a number has significance. For example one can split on age=19 when answering the question "does a person drink on their birthday". However equality based rule does not make sense for this problem as the pattern is more area based (a continues region of lat/long) as opposed to being based on specific longitudes or latitudes.
}

\subsection{Decision Stump Implementation}

\blu{Add a new function ``decisionStump'' to \emph{decisionStump.jl} that finds the best inequality-based rule, and report the updated error you obtain by using inequalities instead of discretizing and testing equality.}

\begin{lstlisting}[language=julia]
	function decisionStump(X,y)
		# Fits a decision stump based on inequality

		# Get the size of the data matrix
		(n,d) = size(X)

		# Initialize the "best rule" with the baseline rule (no split)
		y_mode = mode(y)
		minError = sum(y .!= y_mode);
		splitVariable = [];
		splitValue = [];
		splitLE = y_mode; # predicted label for less than or equal to the split value
		splitGT = [];     # predicted label for greater than the split value

		# Search for the best rule
		# (Uses O(n^2d) approach to keep code simple)
		yhat = zeros(n)
		for j in 1:d
			# Try unique values of column as split values
			for val in unique(X[:,j])

				# Test whether each object satisfies inequality
				lessThanOrEqual = X[:,j] .<= val

				# Find correct label on both sides of split
				y_le = mode(y[lessThanOrEqual])
				y_gt = mode(y[.!lessThanOrEqual])

				# Make predictions
				yhat[lessThanOrEqual] .= y_le
				yhat[.!lessThanOrEqual] .= y_gt

				# Compute error
				trainError = sum(yhat .!= y)

				# Update best rule
				if trainError < minError
					minError = trainError
					splitVariable = j
					splitValue = val
					splitLE = y_le
					splitGT = y_gt
				end
			end
		end

		# Now that we have the best rule,
		# let's build our splitting function
		function split(Xhat)
			(t,d) = size(Xhat)
			if isempty(splitVariable)
				return fill(true,t)
			else
				return (Xhat[:,splitVariable] .<= splitValue)
			end
		end

		# Now that we have the best rule,
		# let's build our predict function
		function predict(Xhat)
			(t,d) = size(Xhat)
			lessThanOrEqual = split(Xhat)
			yhat = fill(splitLE,t)
			if any(.!lessThanOrEqual)
				yhat[.!lessThanOrEqual] .= splitGT
			end
			return yhat
		end

		return StumpModel(predict,split,isempty(splitGT))
	end
\end{lstlisting}

\includegraphics{inequality.png}

\subsection{Constructing Decision Trees}

Once your \emph{decisionStump} function is finished, the script \emph{example\_decisionTree} will be able to fit a decision tree of depth 2 to the same dataset (which results in a lower training error). Look at how the decision tree is stored and how the (recursive) \emph{predict} function works. \blu{Using the same splits as the fitted depth-2 decision tree, write out what an alternate version of the predict function would be for classifying one training example as a simple program using if/else statements (as in the first slide of L3 that has the title ``Decision Trees'').}

\begin{lstlisting}[language=julia]
	function predict(Xhat)
		(t,d) = size(Xhat)
		yhat = zeros(t)
		for i in 1:t
			if Xhat[i,2] <= 37.669007
				if Xhat[i,1] <= -115.577574
					yhat[i] = 1
				else
					yhat[i] = 2
				end
			else
				if Xhat[i,1] <= -96.090109
					yhat[i] = 2
				else
					yhat[i] = 1
				end
			end
		end
		return yhat
	end
\end{lstlisting}

\subsection{Cost of Fitting Decision Trees}

In class, we discussed how in general the decision stump minimizing the classification error can be found in $O(nd\log n)$ time. Using the greedy recursive splitting procedure, \blu{what is the total cost of fitting a decision tree of depth $m$ in terms of $n$, $d$, and $m$?} 

Hint: even thought there could be $(2^m-1)$ decision stumps, keep in mind not every stump will need to go through every example. Note also that we stop growing the decision tree if a node has no examples, so we may not even need to do anything for many of the $(2^m-1)$ decision stumps.

\vspace{25pt}
\textbf{HAVE YOU DOUBLE CHECKED THAT YOU'RE FOLLOWING ALL THE ASSIGNMENT SUBMISSION INSTRUCTIONS???}\\
\url{www.cs.ubc.ca/~schmidtm/Courses/340-F19/assignments.pdf}

\end{document}